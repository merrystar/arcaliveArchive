from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time, os, requests, json, platform
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timezone


# --------- ë³€ìˆ˜ ì„¤ì • ---------
DEBUG = False

CHANNEL_URL = "https://arca.live/b/cook"  # í¬ë¡¤ë§í•  ì•„ì¹´ë¼ì´ë¸Œ ì±„ë„ URL
EMPTY_PAGE_LIMIT = 100  # í•´ë‹¹ í˜ì´ì§€ë™ì•ˆ í¬ë¡¤ë§ ê°€ëŠ¥í•œ ê¸€ì´ ë‚˜ì˜¤ì§€ ì•Šì„ ê²½ìš° ì¢…ë£Œ
SAVE_DIR = "articles"
CSS_DIR = os.path.join(SAVE_DIR, "css")
PROGRESS_FILE = "progress.json"
# -------------------------


current_url = None
last_saved_link = None
downloaded_css = []


def debug_print(*args, **kwargs):
    if DEBUG:
        print(*args, **kwargs)


def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            if "downloaded_css" not in data:
                data["downloaded_css"] = []
            return data
    return {"last_link_number": None, "downloaded_css": []}


def save_progress(last_url, last_link_number, downloaded_css):
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump({
            "last_url": last_url,
            "last_link_number": last_link_number,
            "downloaded_css": downloaded_css
        }, f)


def make_safe_filename(s):
    return "".join(c for c in s if c.isalnum() or c in " _-")[:50]


def save_article(driver, article):
    driver.get(article["link"])
    time.sleep(1)

    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    # ë§í¬ì—ì„œ ê¸€ ë²ˆí˜¸ ì¶”ì¶œ
    link_number = article["link"].rstrip("/").split("/")[-1].split("?")[0]

    # í´ë”ëª…ì— ê¸€ ì œëª© í¬í•¨
    safe_title = make_safe_filename(article["title"])
    article_dir = os.path.join(SAVE_DIR, f"{link_number}_{safe_title}")
    os.makedirs(article_dir, exist_ok=True)

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    for idx, img in enumerate(soup.select("img")):
        src = img.get("src")
        if not src:
            continue
        img_url = urljoin(article["link"], src)
        try:
            res = requests.get(img_url, timeout=10)
            if res.status_code == 200:
                ext = os.path.splitext(img_url.split("?")[0])[1] or ".jpg"
                img_filename = f"img_{idx}{ext}"
                img_path = os.path.join(article_dir, img_filename)
                with open(img_path, "wb") as f:
                    f.write(res.content)
                img["src"] = img_filename
        except:
            continue

    # CSS ë‹¤ìš´ë¡œë“œ (í•œ ë²ˆë§Œ)
    for idx, css in enumerate(soup.select("link[rel='stylesheet']")):
        href = css.get("href")
        if not href:
            continue
        css_url = urljoin(CHANNEL_URL, href)
        css_name = os.path.basename(css_url.split("?")[0])
        if css_name in downloaded_css:
            css["href"] = os.path.join("..", "css", css_name).replace("\\", "/")
            continue

        css_path = os.path.join(CSS_DIR, css_name)
        try:
            res = requests.get(css_url, timeout=10)
            if res.status_code == 200:
                with open(css_path, "w", encoding="utf-8") as f:
                    f.write(res.text)
                css["href"] = os.path.join("..", "css", css_name).replace("\\", "/")
                downloaded_css.add(css_name)
                save_progress(last_url=current_url, last_link_number=last_saved_link,
                              downloaded_css=list(downloaded_css))
        except:
            continue

    # ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
    for idx, video in enumerate(soup.select("video")):
        video_src = video.get("src")
        sources = video.find_all("source")
        urls = [video_src] if video_src else []
        urls += [s.get("src") for s in sources if s.get("src")]

        for vid_idx, url in enumerate(urls):
            if not url:
                continue
            video_url = urljoin(article["link"], url)
            try:
                res = requests.get(video_url, timeout=20)
                if res.status_code == 200:
                    ext = os.path.splitext(video_url.split("?")[0])[1] or ".mp4"
                    video_filename = f"video_{idx}_{vid_idx}{ext}"
                    video_path = os.path.join(article_dir, video_filename)
                    with open(video_path, "wb") as f:
                        f.write(res.content)
                    # HTML ë‚´ ê²½ë¡œë¥¼ ë¡œì»¬ ìƒëŒ€ê²½ë¡œë¡œ ë³€ê²½
                    if video_src:
                        video["src"] = video_filename
                    for s in sources:
                        if s.get("src") == url:
                            s["src"] = video_filename
            except:
                continue

    # HTML ì €ì¥
    html_filename = "index.html"
    html_path = os.path.join(article_dir, html_filename)
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(str(soup))

    print(f"âœ… Saved {html_path}")

    # ì§„í–‰ ìƒí™© ê¸°ë¡
    save_progress(last_url=current_url, last_link_number=link_number, downloaded_css=list(downloaded_css))


def parse_page(driver):
    rows = driver.find_elements(By.CSS_SELECTOR, "a.vrow.column:not(.notice)")
    articles = []

    for row in rows:
        try:
            title_elem = row.find_element(By.CSS_SELECTOR, ".vcol.col-title")
            if "(æ¨©é™ãªã—)" in title_elem.text or "(ê¶Œí•œ ì—†ìŒ)" in title_elem.text:
                continue

            link = row.get_attribute("href")
            articles.append({
                "link": link,
                "title": title_elem.find_element(By.CSS_SELECTOR, ".title").text.strip()
            })
        except:
            continue
    return articles


def find_next_page_url(driver):
    try:
        WebDriverWait(driver, 5).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.page-item"))
        )
    except Exception as e:
        debug_print(f"[ì§„ë‹¨] í˜ì´ì§€ë„¤ì´ì…˜(li.page-item) ìì²´ë¥¼ ëª» ì°¾ìŒ: {repr(e)}")
        return None

    links = driver.find_elements(By.CSS_SELECTOR, "li.page-item a.page-link")
    debug_print(f"[ì§„ë‹¨] a.page-link ê°œìˆ˜: {len(links)}")

    # --- 1ë‹¨ê³„: +1 ë§í¬ ì°¾ê¸° ---
    for a in links:
        try:
            text = (a.text or "").strip()
            if text == "+1":
                href = a.get_attribute("href")
                if href:
                    debug_print(f"[ì§„ë‹¨] +1 ë§í¬ ë°œê²¬: {href}")
                    return urljoin(CHANNEL_URL, href)
        except:
            continue

    # --- 2ë‹¨ê³„: ion-chevron-right í•œ ê°œë§Œ í¬í•¨ ë§í¬ ---
    right_candidates = []
    for i, a in enumerate(links):
        try:
            spans = a.find_elements(By.TAG_NAME, "span")
            text = (a.text or "").strip()
            span_classes = [s.get_attribute("class") or "" for s in spans]
            href = a.get_attribute("href")

            debug_print(f"[ì§„ë‹¨] ë§í¬#{i}: text='{text}', span_class={span_classes}, href='{href}'")

            cond_single_span = (len(spans) == 1)
            cond_right_icon = any("ion-chevron-right" in sc for sc in span_classes)
            cond_no_text = (text == "")

            if cond_single_span and cond_right_icon and cond_no_text:
                right_candidates.append(a)
        except:
            continue

    if right_candidates:
        href = right_candidates[0].get_attribute("href")
        debug_print(f"[ì§„ë‹¨] ion-chevron-right ë§í¬ ë°œê²¬: {href}")
        return urljoin(CHANNEL_URL, href)

    debug_print("[ì§„ë‹¨] ë‹¤ìŒ í˜ì´ì§€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨.")
    return None


def main():
    edge_options = Options()
    edge_options.add_experimental_option("detach", True)
    driver_path = "msedgedriver.exe" if platform.system().lower() == "windows" else "./msedgedriver"

    service = Service(driver_path)
    driver = webdriver.Edge(service=service, options=edge_options)

    driver.get(CHANNEL_URL)
    print("âš ï¸ ë¸Œë¼ìš°ì €ì—ì„œ ë¡œê·¸ì¸ í›„ ì—”í„°ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”... (QR ë¡œê·¸ì¸ ê¶Œì¥)")
    input()

    now = datetime.now(timezone.utc)
    start_url = f"{CHANNEL_URL}?before={now.strftime('%Y-%m-%dT%H%%3A%M%%3A%SZ')}"

    progress = load_progress()
    current_url = progress.get("last_url", start_url)
    last_saved_link = progress.get("last_link_number", None)
    downloaded_css = set(progress.get("downloaded_css", []))

    os.makedirs(SAVE_DIR, exist_ok=True)
    os.makedirs(CSS_DIR, exist_ok=True)

    driver.get(current_url)
    empty_count = 0

    while True:
        time.sleep(0.5)
        save_progress(last_url=current_url, last_link_number=last_saved_link, downloaded_css=list(downloaded_css))

        print(f"ğŸ“– í˜ì´ì§€ {current_url} í¬ë¡¤ë§ ì¤‘...")
        articles = parse_page(driver)

        if not articles:
            empty_count += 1
            if empty_count >= EMPTY_PAGE_LIMIT:
                print(f"{EMPTY_PAGE_LIMIT} í˜ì´ì§€ ì—°ì†ìœ¼ë¡œ ìƒˆ ê¸€ ì—†ìŒ. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
        else:
            empty_count = 0
            for art in articles:
                save_article(driver, art)
                last_saved_link = art["link"].rstrip("/").split("/")[-1].split("?")[0]

        # --- ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸° (ì§„ë‹¨ ë©”ì‹œì§€ í¬í•¨) ---
        next_url = find_next_page_url(driver)
        if next_url:
            current_url = next_url
            driver.get(current_url)
            continue
        else:
            print("âŒ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    print("ğŸ‰ ëª¨ë“  ê²Œì‹œë¬¼ ì €ì¥ ì™„ë£Œ")


if __name__ == "__main__":
    main()